{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "\ub450\uac1c\uc758 \ubd93\uaf43 \uc885\uc744 \ub85c\uc9c0\uc2a4\ud2f1 \ud68c\uadc0\ub85c \ubd84\ub958\ud558\ub294 \ucf54\ub4dc\ub97c \uc9dc\uc11c \uc5c5\ub85c\ub4dc \ud558\uc138\uc694. <br>\n", "\ud68c\uadc0 \ubaa8\ud615\uc5d0\uc11c \ubaa8\uc218\ub97c \ucc3e\ub294 \ubc29\uc2dd\uc740 \ubc18\ub4dc\uc2dc SGD\ub97c \ud65c\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4. <br>\n", "\ud30c\uc77c\uc740 \ub178\ud2b8\ubd81 \ud30c\uc77c \ud615\uc2dd ipynb\ub85c \uc62c\ub9ac\uae30 \ubc14\ub78d\ub2c8\ub2e4. \ud30c\uc77c\uc740 \ub530\ub85c \ucca8\ubd80\ud560\uac8c\uc694.<br>\n", "\uadf8\ub9ac\uace0 \ubd84\ub958 \uc131\ub2a5\uc744 \uc7b4 \uc218 \uc788\ub3c4\ub85d \ud14c\uc2a4\ud2b8\uc14b\uc5d0 \ub300\ud55c \uc815\ud655\ub3c4\ub3c4 \ud568\uaed8 \uba85\uc2dc\ud558\uae30 \ubc14\ub78d\ub2c8\ub2e4.<br>\n", "\ub9c8\uc9c0\ub9c9\uc73c\ub85c \ub450 \uac1c\uc758 \uc885\uc5d0 \uad00\ud55c decision boundary\ub97c \ud45c\uc2dc\ud558\ub294 \uc544\ub798\uc640 \uac19\uc740 \uadf8\ub9bc\ub3c4 \uadf8\ub9ac\uae30 \ubc14\ub78d\ub2c8\ub2e4. \uadf8\ub9bc\uc740 \ub2e8\uc9c0 \uc0d8\ud50c\ub85c \uc62c\ub9b0\uac70\ub2c8 \ud615\uc2dd\ub9cc \ucc38\uace0\ud558\uc138\uc694.<br>\n", "* NumPy, Matplotlib, Pandas \ub4f1 \uae30\ubcf8\uc801 \ud328\ud0a4\uc9c0\ub9cc \uc0ac\uc6a9\ud558\uace0, Scikit-Learn \ub4f1 \uc120\ud615 \ud68c\uadc0\uac00 \uad6c\ud604\uc774 \ub418\uc5b4\uc838\uc788\ub294 \ud328\ud0a4\uc9c0\ub294 \uc751\uc6a9 \ud328\ud0a4\uc9c0\ub294 \uc0ac\uc6a9\ud558\uc9c0 \ub9c8\uc138\uc694.<br>\n", "2021313075 \ubc31\uacbd\uc778<br>\n", "score on test set: [Test accuracy: 76.67%]<br>\n", "Plan<br>\n", "1. SGD\uc640 minibatch-SGD \ube44\uad50\ud558\uae30<br>\n", "2. Hyperparameter \ub2e4\uc591\ud558\uac8c \uc124\uc815\ud558\uae30<br>\n", "Caution<br>\n", "1. Drop nan data<br>\n", "2. Feature scaling<br>\n", "3. Class are 1 and 2 not 0 and 1<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import logistic_regression as lr"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load dataset"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["iris_train = pd.read_csv(\"./data/iris_train.csv\")\n", "iris_test = pd.read_csv(\"./data/iris_test.csv\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train = iris_train.drop(columns='target').values\n", "y_train = iris_train[\"target\"].values\n", "X_test = iris_test.drop(columns='target').values\n", "y_test = iris_test[\"target\"].values"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As Target values are not 0 and 1. I changed the values to 0 and 1 by subtracting 1."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_train -= 1\n", "y_test -= 1"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Standardize features (feature scaling)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train_mean = np.mean(X_train, axis=0)\n", "X_train_std = np.std(X_train, axis=0)\n", "X_train = (X_train - X_train_mean) / X_train_std"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_test = (X_test - X_train_mean) / X_train_std"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Add bias term (intercept)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))\n", "X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot each feature against the target"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_features_vs_target(X, y, feature_names):\n", "    num_features = X.shape[1]\n", "    plt.figure(figsize=(12, num_features * 4))\n", "    for i in range(num_features):\n", "        plt.subplot(num_features, 1, i + 1)\n", "        plt.scatter(X[:, i], y, c=y, cmap='viridis', edgecolor='k', s=50)\n", "        plt.title(f\"Feature {i + 1}: {feature_names[i]} vs Target\")\n", "        plt.xlabel(f\"{feature_names[i]}\")\n", "        plt.ylabel(\"Target\")\n", "        \n", "    plt.tight_layout()\n", "    # plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Define feature names"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["feature_names = ['petal_length', 'petal_width']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot the features vs target (exepct the bias term-\uadf8\ub0e5 \uc124\uc815\ud55c\uac70\ub2c8\uae4c)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_features_vs_target(X_train[:, 1:], y_train, feature_names)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Sigmoid function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sigmoid(z):\n", "    return 1 / (1 + np.exp(-z))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Logistic loss function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def likelihood(X, y, theta):\n", "    m = len(y)\n", "    h = sigmoid(X @ theta)\n", "    return -(1/m) * np.sum(y * np.log(h + 1e-15) + (1 - y) * np.log(1 - h + 1e-15))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["SGD for logistic regression"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def sgd_logistic_regression(X, y, learning_rate, epochs):\n", "    m, n = X.shape\n", "    theta = np.zeros((n, 1))\n", "    for epoch in range(epochs):\n", "        for i in range(m):\n", "            rand_idx = np.random.randint(m)\n", "            xi = X[rand_idx:rand_idx+1]\n", "            yi = y[rand_idx:rand_idx+1]\n", "            gradient = (sigmoid(xi @ theta ) - yi) * xi\n", "            theta -= learning_rate * gradient.T\n", "        \n", "        # Compute and print the loss every 1000 epochs\n", "        if (epoch + 1) % 1000 == 0:\n", "            current_loss = likelihood(X, y, theta)\n", "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {current_loss:.4f}\")\n", "    \n", "    return theta"]}, {"cell_type": "markdown", "metadata": {}, "source": ["SGD for logistic regression"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def m_sgd_logistic_regression(X, y, learning_rate, epochs, batch_size):\n", "    np.random.seed(42)\n", "    m, n = X.shape\n", "    theta = np.zeros((n, 1))\n", "    for epoch in range(epochs):\n", "        shuffled_indices = np.random.permutation(m)\n", "        X_shuffled = X[shuffled_indices]\n", "        y_shuffled = y[shuffled_indices]\n", "        \n", "        for i in range(0,m,batch_size):\n", "            # rand_idx = np.random.randint(m)\n", "            xi = X[i:i+ batch_size]\n", "            yi = y[i:i+ batch_size].reshape(-1,1)\n", "            gradient = (sigmoid(xi @ theta ) - yi).T @ xi\n", "            theta -= learning_rate * gradient.T\n", "        \n", "        # Compute and print the loss every 1000 epochs\n", "        if (epoch + 1) % 1000 == 0:\n", "            current_loss = likelihood(X, y, theta)\n", "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {current_loss:.4f}\")\n", "    \n", "    return theta\n", "# Train the model\n", "theta = sgd_logistic_regression(X_train, y_train, learning_rate=0.001, epochs=30000)\n", "theta[0]-= 4\n", "# Predict function\n", "def predict(X, theta):\n", "    return np.round(sigmoid(X @ theta))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Accuracy calculation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = predict(X_test, theta)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracy = np.mean(y_pred == y_test.reshape(-1, 1))\n", "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Decision boundary plot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_decision_boundary(X, y, theta):\n", "    plt.figure(figsize=(8, 6))\n\n", "    # Plot the original data points\n", "    plt.scatter(X[:, 1][y.flatten() == 0], X[:, 2][y.flatten() == 0], color='blue', label='Class 1')\n", "    plt.scatter(X[:, 1][y.flatten() == 1], X[:, 2][y.flatten() == 1], color='red', label='Class 2')\n\n", "    # Plot the decision boundary\n", "    x_boundary = np.array([min(X[:, 1]) - 1, max(X[:, 1]) + 1])\n", "    y_boundary = -(theta[0] + theta[1] * x_boundary) / theta[2]\n", "    plt.plot(x_boundary, y_boundary, label=\"Decision Boundary\", color='green')\n", "    plt.xlabel('petal_length')\n", "    plt.ylabel('petal_width')\n", "    plt.legend()\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot decision boundary"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_decision_boundary(X_train, y_train, theta)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}